{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         768000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 64)          41472     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 810,793\n",
      "Trainable params: 810,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 8s 210us/sample - loss: 0.4103 - acc: 0.8142 - val_loss: 0.3046 - val_acc: 0.8692\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 7s 170us/sample - loss: 0.2606 - acc: 0.8936 - val_loss: 0.3071 - val_acc: 0.8721\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 7s 170us/sample - loss: 0.2163 - acc: 0.9155 - val_loss: 0.3017 - val_acc: 0.8759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20192b7d8c8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Bidirectional, Dense, Embedding, CuDNNLSTM, Dropout, GlobalMaxPool1D\n",
    "\n",
    "#Get dataset from local directory\n",
    "df = pd.read_csv('D:/Programacion/Datasets/IMDB/IMDB_Dataset.csv', encoding='utf-8')\n",
    "print(df.head())\n",
    "\n",
    "#Set the dataset labels to have a numerical value\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df['sentiment'].values)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['review'].values, labels, test_size=0.2)\n",
    "\n",
    "#Hyperparameters\n",
    "max_features = 6000 #n_words in the embedding dictionary\n",
    "max_words = 130 #n_words per review\n",
    "input_dim = max_features #embedding layer input\n",
    "embedding_dim = 128 #embedding layer output\n",
    "epochs = 3\n",
    "batch_size = 100\n",
    "\n",
    "#Keras tokenizer\n",
    "tokenizer_obj = Tokenizer(num_words = max_features)\n",
    "tokenizer_obj.fit_on_texts(x_train)\n",
    "\n",
    "#Fit tokenizer and turn texts into sequences of numbers\n",
    "x_train = tokenizer_obj.texts_to_sequences(x_train)\n",
    "x_test = tokenizer_obj.texts_to_sequences(x_test)\n",
    "x_train = pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = pad_sequences(x_test, maxlen=max_words)\n",
    "\n",
    "#Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim, embedding_dim))\n",
    "model.add(Bidirectional(CuDNNLSTM(32, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#Compile\n",
    "model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#Fit and evaluate\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.0_gpu",
   "language": "python",
   "name": "tf1.0_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
